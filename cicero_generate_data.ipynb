{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from cltk import NLP\n",
    "from cltk.data.fetch import FetchCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_modules.text_prep import read_texts, get_lemmata\n",
    "from my_modules.sentiment_analysis import get_sentiment_lex, analyze_sentiment\n",
    "from my_modules.data_handling import save_embeddings_csv, save_lemmata_csv, save_scores_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Latin Library Texts form CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Latin Corpus\n",
    "corpus_downloader = FetchCorpus(language=\"lat\")\n",
    "#Download Latin Library text\n",
    "corpus_downloader.import_corpus(\"lat_text_latin_library\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cicero Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and read speeches/files, return dicitonary\n",
    "cicero = read_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NLP Pipeline to Analyze Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the default Pipeline for Latin\n",
    "cltk_nlp = NLP(language=\"lat\")\n",
    "\n",
    "#Now execute NLP algorithms upon all input texts at once\n",
    "#Remove ``LatinLexiconProcess` b/c it is slow and not required\n",
    "cltk_nlp.pipeline.processes.pop(-1)\n",
    "\n",
    "all_docs = {}\n",
    "for filename in cicero: #for each speech\n",
    "    #analyze speech\n",
    "    cltk_doc = cltk_nlp.analyze(text=cicero[filename])\n",
    "    #remove .txt in name\n",
    "    title = filename.replace(\".txt\", \"\")\n",
    "    #save analyzed speech in dict\n",
    "    all_docs[title] = cltk_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Negative Words, Frequencies, Lex Divs, and Embeddings and Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dictionary of sentiment lexicon\n",
    "lex = get_sentiment_lex()\n",
    "lexical_diversities = {}\n",
    "all_scores = {-1: [], -0.5: [], 0: [], 0.5: [], 1: []}\n",
    "for speech in all_docs: #for each speech\n",
    "    doc = all_docs[speech]\n",
    "    #get list of Words in doc\n",
    "    words = doc.words\n",
    "    #Get list of lemmata for each word in doc\n",
    "    lemmata = get_lemmata(words)\n",
    "    #save these lemmata for current speech\n",
    "    save_lemmata_csv(lemmata, speech)\n",
    "    #get dictionary of words for each sentiment score (-1, -0.5, 0, 0.5, 1)\n",
    "    scores = analyze_sentiment(lex, lemmata, words, doc)\n",
    "    #save these scores for current speech\n",
    "    save_scores_csv(scores, speech)\n",
    "    #save embeddings for each word in current speech  \n",
    "    save_embeddings_csv(lemmata, words, speech)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27d4a6d341bcd334d69a27691afbfa6e6ad7f73bfcdfc17ecc6c5af029c4210a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}